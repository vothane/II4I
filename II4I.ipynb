{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "II4I.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/II4I/blob/master/II4I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK-Hm5YzM_Y2",
        "colab_type": "text"
      },
      "source": [
        "**Lets go Bruins. All eyes on center.**\n",
        "\n",
        "![](https://https://media1.giphy.com/media/hJjTwuJjjsVgc/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztdTbEQpNQrI",
        "colab_type": "text"
      },
      "source": [
        "![](https://media1.giphy.com/media/hJjTwuJjjsVgc/giphy.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRX3bXfNXzZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF7mxvWpYyMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Functions for Capsule Networks\n",
        "def squash(vectors, name=None):\n",
        "    \"\"\"\n",
        "    Squashing Function\n",
        "    :parameter vectors: vector input that needs to be squashed\n",
        "    :parameter name: Name of the tensor on the graph\n",
        "    :return: a tensor with same shape as vectors but squashed as mentioned in the paper\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, default_name=\"squash_op\"):\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=-2, keepdims=True)\n",
        "        scale = s_squared_norm / (1. + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
        "        return scale*vectors\n",
        "\n",
        "\n",
        "def routing(u):\n",
        "    \"\"\"\n",
        "    This function performs the routing algorithm\n",
        "    :parameter u: Input tensor with [batch_size, num_caps_input_layer=1152, 1, caps_dim_input_layer=8, 1] shape.\n",
        "                NCAPS_CAPS1: num capsules in the PrimaryCaps layer l\n",
        "                CAPS_DIM_CAPS2: dimensions of output vectors of Primary caps layer l\n",
        "    :return: \"v_j\" vector (tensor) in Digitcaps Layer\n",
        "             Shape:[batch_size, NCAPS_CAPS1=10, CAPS_DIM_CAPS2=16, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    #local variable b_ij: [batch_size, num_caps_input_layer=1152, num_caps_output_layer=10, 1, 1]\n",
        "                #num_caps_output_layer: number of capsules in Digicaps layer l+1\n",
        "    b_ij = tf.zeros([BATCH_SIZE, NCAPS_CAPS1, NCAPS_CAPS2, 1, 1], dtype=np.float32, name=\"b_ij\")\n",
        "\n",
        "    # Preparing the input Tensor for total number of DigitCaps capsule for multiplication with W\n",
        "    u = tf.tile(u, [1, 1, b_ij.shape[2].value, 1, 1])   # u => [batch_size, 1152, 10, 8, 1]\n",
        "\n",
        "\n",
        "    # W: [num_caps_input_layer, num_caps_output_layer, len_u_i, len_v_j] as mentioned in the paper\n",
        "    W = tf.get_variable('W', shape=(1, u.shape[1].value, b_ij.shape[2].value, u.shape[3].value, CAPS_DIM_CAPS2),\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=STDEV))\n",
        "    W = tf.tile(W, [BATCH_SIZE, 1, 1, 1, 1]) # W => [batch_size, 1152, 10, 8, 16]\n",
        "\n",
        "    #Computing u_hat (as mentioned in the paper)\n",
        "    u_hat = tf.matmul(W, u, transpose_a=True)  # [batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat;\n",
        "    # In backward pass, no gradient pass from  u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='gradient_stop')\n",
        "\n",
        "    # Routing Algorithm Begins here\n",
        "    for r in range(ROUTING_ITERATIONS):\n",
        "        with tf.variable_scope('iterations_' + str(r)):\n",
        "            c_ij = tf.nn.softmax(b_ij, axis=2) # [batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to back propagate gradient\n",
        "            if r == ROUTING_ITERATIONS - 1:\n",
        "                s_j = tf.multiply(c_ij, u_hat) # [batch_size, 1152, 10, 16, 1]\n",
        "                # then sum as per paper\n",
        "                s_j = tf.reduce_sum(s_j, axis=1, keep_dims=True) # [batch_size, 1, 10, 16, 1]\n",
        "\n",
        "                v_j = squash(s_j) # [batch_size, 1, 10, 16, 1]\n",
        "\n",
        "            elif r < ROUTING_ITERATIONS - 1:  # No backpropagation in these iterations\n",
        "                s_j = tf.multiply(c_ij, u_hat_stopped)\n",
        "                s_j = tf.reduce_sum(s_j, axis=1, keepdims=True)\n",
        "                v_j = squash(s_j)\n",
        "                v_j = tf.tile(v_j, [1, u.shape[1].value, 1, 1, 1]) # [batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "                # Multiplying in last two dimensions: [16, 1]^T x [16, 1] yields [1, 1]\n",
        "                u_hat_dot_v = tf.matmul(u_hat_stopped, v_j, transpose_a=True) # [batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                b_ij = tf.add(b_ij,u_hat_dot_v)\n",
        "    return tf.squeeze(v_j, axis=1) # [batch_size, 10, 16, 1]\n",
        "\n",
        "  \n",
        "def shuffle_data(x, y):\n",
        "    \"\"\" Shuffle the features and labels of input data\"\"\"\n",
        "    perm = np.arange(y.shape[0])\n",
        "    np.random.shuffle(perm)\n",
        "    shuffle_x = x[perm,:,:,:]\n",
        "    shuffle_y = y[perm]\n",
        "    return shuffle_x, shuffle_y\n",
        "\n",
        "\n",
        "def load_existing_details():\n",
        "    \"\"\"\n",
        "    This function loads the train and val files to continue training\n",
        "    :return: handles to train and val files and minimum validation loss\n",
        "    \"\"\"\n",
        "    train_path = RESULTS_DIR  + '/' + 'train.csv'\n",
        "    val_path = RESULTS_DIR + '/' + 'validation.csv'\n",
        "    # finding the minimum validation loss so far\n",
        "    f_val = open(val_path, 'r')\n",
        "    lines = f_val.readlines()\n",
        "    data = np.genfromtxt(lines[-1:], delimiter=',')\n",
        "    min_loss = np.min(data[1:, 2])\n",
        "    # loading the train and val files to continue training\n",
        "    train_file = open(train_path, 'a')\n",
        "    val_file = open(val_path, 'a')\n",
        "    return train_file, val_file, min_loss\n",
        "\n",
        "\n",
        "def eval_performance(sess, model, x, y):\n",
        "    '''\n",
        "    This function is mainly used to evaluate the accuracy on test and validation sets\n",
        "    :param sess: session\n",
        "    :param model: model to be used\n",
        "    :param x: images\n",
        "    :param y: labels\n",
        "    :return: returns the average accuracy and loss for the dataset\n",
        "    '''\n",
        "    acc_all = loss_all = np.array([])\n",
        "    num_batches = int(y.shape[0] / BATCH_SIZE)\n",
        "    for batch_num in range(num_batches):\n",
        "        start = batch_num * BATCH_SIZE\n",
        "        end = start + BATCH_SIZE\n",
        "        x_batch, y_batch = x[start:end], y[start:end]\n",
        "        acc_batch, loss_batch, prediction_batch = sess.run([model.accuracy, model.combined_loss, model.y_predicted],\n",
        "                                                     feed_dict={model.X: x_batch, model.Y: y_batch})\n",
        "        acc_all = np.append(acc_all, acc_batch)\n",
        "        loss_all = np.append(loss_all, loss_batch)\n",
        "    return np.mean(acc_all), np.mean(loss_all)\n",
        "\n",
        "def reconstruction(x, y, decoder_output, y_pred, n_samples):\n",
        "    '''\n",
        "    This function is used to reconstruct sample images for analysis\n",
        "    :param x: Images\n",
        "    :param y: Labels\n",
        "    :param decoder_output: output from decoder\n",
        "    :param y_pred: predictions from the model\n",
        "    :param n_samples: num images\n",
        "    :return: saves the reconstructed images\n",
        "    '''\n",
        "\n",
        "    sample_images = x.reshape(-1, IMG_WIDTH, IMG_HEIGHT)\n",
        "    decoded_image = decoder_output.reshape([-1, IMG_WIDTH, IMG_WIDTH])\n",
        "\n",
        "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(1, n_samples, i+ 1)\n",
        "        plt.imshow(sample_images[i], cmap=\"binary\")\n",
        "        plt.title(\"Label:\" + IMAGE_LABELS[np.argmax(y[i])])\n",
        "        plt.axis(\"off\")\n",
        "    fig.savefig(RESULTS_DIR + '/' + 'input_images.png')\n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(1, n_samples, i + 1)\n",
        "        plt.imshow(decoded_image[i], cmap=\"binary\")\n",
        "        plt.title(\"Prediction:\" + IMAGE_LABELS[y_pred[i]])\n",
        "        plt.axis(\"off\")\n",
        "    fig.savefig(RESULTS_DIR + '/' + 'decoder_images.png')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jMVKE4YWu81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet:\n",
        "    def __init__(self):\n",
        "        with tf.variable_scope('Input'):\n",
        "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS], dtype=tf.float32, name=\"X\")\n",
        "            self.Y = tf.placeholder(shape=[None, N_CLASSES], dtype=tf.float32, name=\"Y\")\n",
        "            self.mask_with_labels = tf.placeholder_with_default(False, shape=(), name=\"mask_with_labels\")\n",
        "\n",
        "        self.define_network()\n",
        "        self.define_loss()\n",
        "        self.define_accuracy()\n",
        "        self.define_optimizer()\n",
        "        self.summary_()\n",
        "\n",
        "    def define_network(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            conv1_layer = tf.layers.conv2d(self.X, name=\"conv1_layer\", **CONV1_LAYER_PARAMS) # [batch_size, 20, 20, 256]\n",
        "\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            conv2_layer = tf.layers.conv2d(conv1_layer, name=\"conv2_layer\", **CONV2_LAYER_PARAMS) # [batch_size, 6, 6, 256]\n",
        "\n",
        "            primary_caps = tf.reshape(conv2_layer, (BATCH_SIZE, NCAPS_CAPS1, CAPS_DIM_CAPS1, 1), name=\"primary_caps\") # [batch_size, 1152, 8, 1]\n",
        "            primary_caps_output = squash(primary_caps, name=\"caps1_output\")\n",
        "            # [batch_size, 1152, 8, 1]\n",
        "\n",
        "        # DigitCaps layer, return [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitcaps_input = tf.reshape(primary_caps_output, shape=(BATCH_SIZE, NCAPS_CAPS1, 1, CAPS_DIM_CAPS1, 1)) # [batch_size, 1152, 1, 8, 1]\n",
        "            # [batch_size, 1152, 10, 1, 1]\n",
        "            self.digitcaps_output = routing(digitcaps_input) # [batch_size, 10, 16, 1]\n",
        "\n",
        "        # Decoder\n",
        "        with tf.variable_scope('Masking'):\n",
        "            self.v_norm = tf.sqrt(tf.reduce_sum(tf.square(self.digitcaps_output), axis=2, keep_dims=True) + tf.keras.backend.epsilon())\n",
        "\n",
        "            predicted_class = tf.to_int32(tf.argmax(self.v_norm, axis=1)) #[batch_size, 10,1,1]\n",
        "            self.y_predicted = tf.reshape(predicted_class, shape=(BATCH_SIZE,))  #[batch_size]\n",
        "            y_predicted_one_hot = tf.one_hot(self.y_predicted, depth=NCAPS_CAPS2)  #[batch_size,10]  One hot operation\n",
        "\n",
        "            reconstruction_targets = tf.cond(self.mask_with_labels,  # condition\n",
        "                                      lambda: self.Y,  # if True (Training)\n",
        "                                      lambda: y_predicted_one_hot,  # if False (Test)\n",
        "                                      name=\"reconstruction_targets\")\n",
        "\n",
        "            digitcaps_output_masked = tf.multiply(tf.squeeze(self.digitcaps_output), tf.expand_dims(reconstruction_targets, -1)) # [batch_size, 10, 16]\n",
        "\n",
        "\n",
        "            #Flattening as suggested by the paper\n",
        "            decoder_input = tf.reshape(digitcaps_output_masked, [BATCH_SIZE, -1]) # [batch_size, 160]\n",
        "\n",
        "\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            fc1 = tf.layers.dense(decoder_input, layer1_size, activation=tf.nn.relu, name=\"FC1\") # [batch_size, 512]\n",
        "            fc2 = tf.layers.dense(fc1, layer2_size, activation=tf.nn.relu, name=\"FC2\") # [batch_size, 1024]\n",
        "            self.decoder_output = tf.layers.dense(fc2, output_size, activation=tf.nn.sigmoid, name=\"FC3\") # [batch_size, 784]\n",
        "\n",
        "\n",
        "    def define_loss(self):\n",
        "        # Margin Loss\n",
        "        with tf.variable_scope('Margin_Loss'):\n",
        "            # max(0, m_plus-||v_c||)^2\n",
        "            positive_error = tf.square(tf.maximum(0., 0.9 - self.v_norm)) # [batch_size, 10, 1, 1]\n",
        "            # max(0, ||v_c||-m_minus)^2\n",
        "            negative_error = tf.square(tf.maximum(0., self.v_norm - 0.1)) # [batch_size, 10, 1, 1]\n",
        "            # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "            positive_error = tf.reshape(positive_error, shape=(BATCH_SIZE, -1))\n",
        "            negative_error = tf.reshape(negative_error, shape=(BATCH_SIZE, -1))\n",
        "\n",
        "            Loss_vec = self.Y * positive_error + 0.5 * (1- self.Y) * negative_error # [batch_size, 10]\n",
        "            self.margin_loss = tf.reduce_mean(tf.reduce_sum(Loss_vec, axis=1), name=\"margin_loss\")\n",
        "\n",
        "        # Reconstruction Loss\n",
        "        with tf.variable_scope('Reconstruction_Loss'):\n",
        "            ground_truth = tf.reshape(self.X, shape=(BATCH_SIZE, -1))\n",
        "            self.reconstruction_loss = tf.reduce_mean(tf.square(self.decoder_output - ground_truth))\n",
        "\n",
        "        # Combined Loss\n",
        "        with tf.variable_scope('Combined_Loss'):\n",
        "            self.combined_loss = self.margin_loss + 0.0005 * self.reconstruction_loss\n",
        "\n",
        "    def define_accuracy(self):\n",
        "        with tf.variable_scope('Accuracy'):\n",
        "            correct_predictions = tf.equal(tf.to_int32(tf.argmax(self.Y, axis=1)), self.y_predicted)\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "    def define_optimizer(self):\n",
        "        with tf.variable_scope('Optimizer'):\n",
        "            optimizer = tf.train.AdamOptimizer()\n",
        "            self.train_optimizer = optimizer.minimize(self.combined_loss, name=\"training_optimizer\")\n",
        "\n",
        "    def summary_(self):\n",
        "        reconstructed_image = tf.reshape(self.decoder_output, shape=(BATCH_SIZE, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS))\n",
        "        summary_list = [tf.summary.scalar('Loss/margin_loss', self.margin_loss),\n",
        "                        tf.summary.scalar('Loss/reconstruction_loss', self.reconstruction_loss),\n",
        "                        tf.summary.image('original', self.X),\n",
        "                        tf.summary.image('reconstructed', reconstructed_image)]\n",
        "        self.summary_ = tf.summary.merge(summary_list)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VunOZJSqWnJa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}