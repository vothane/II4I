{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "II4I.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/II4I/blob/master/II4I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK-Hm5YzM_Y2",
        "colab_type": "text"
      },
      "source": [
        "**Lets go Bruins. All eyes on center.**\n",
        "\n",
        "![](https://https://media1.giphy.com/media/hJjTwuJjjsVgc/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztdTbEQpNQrI",
        "colab_type": "text"
      },
      "source": [
        "![](https://media1.giphy.com/media/hJjTwuJjjsVgc/giphy.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRX3bXfNXzZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as t\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF7mxvWpYyMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Functions for Capsule Networks\n",
        "def squash(vectors, name=None):\n",
        "    \"\"\"\n",
        "    Squashing Function\n",
        "    :parameter vectors: vector input that needs to be squashed\n",
        "    :parameter name: Name of the tensor on the graph\n",
        "    :return: a tensor with same shape as vectors but squashed as mentioned in the paper\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, default_name=\"squash_op\"):\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=-2, keepdims=True)\n",
        "        scale = s_squared_norm / (1. + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
        "        return scale*vectors\n",
        "\n",
        "\n",
        "def routing(u):\n",
        "    \"\"\"\n",
        "    This function performs the routing algorithm\n",
        "    :parameter u: Input tensor with [batch_size, num_caps_input_layer=1152, 1, caps_dim_input_layer=8, 1] shape.\n",
        "                NCAPS_CAPS1: num capsules in the PrimaryCaps layer l\n",
        "                CAPS_DIM_CAPS2: dimensions of output vectors of Primary caps layer l\n",
        "    :return: \"v_j\" vector (tensor) in Digitcaps Layer\n",
        "             Shape:[batch_size, NCAPS_CAPS1=10, CAPS_DIM_CAPS2=16, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    #local variable b_ij: [batch_size, num_caps_input_layer=1152, num_caps_output_layer=10, 1, 1]\n",
        "                #num_caps_output_layer: number of capsules in Digicaps layer l+1\n",
        "    b_ij = tf.zeros([BATCH_SIZE, NCAPS_CAPS1, NCAPS_CAPS2, 1, 1], dtype=np.float32, name=\"b_ij\")\n",
        "\n",
        "    # Preparing the input Tensor for total number of DigitCaps capsule for multiplication with W\n",
        "    u = tf.tile(u, [1, 1, b_ij.shape[2].value, 1, 1])   # u => [batch_size, 1152, 10, 8, 1]\n",
        "\n",
        "\n",
        "    # W: [num_caps_input_layer, num_caps_output_layer, len_u_i, len_v_j] as mentioned in the paper\n",
        "    W = tf.get_variable('W', shape=(1, u.shape[1].value, b_ij.shape[2].value, u.shape[3].value, CAPS_DIM_CAPS2),\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=STDEV))\n",
        "    W = tf.tile(W, [BATCH_SIZE, 1, 1, 1, 1]) # W => [batch_size, 1152, 10, 8, 16]\n",
        "\n",
        "    #Computing u_hat (as mentioned in the paper)\n",
        "    u_hat = tf.matmul(W, u, transpose_a=True)  # [batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat;\n",
        "    # In backward pass, no gradient pass from  u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='gradient_stop')\n",
        "\n",
        "    # Routing Algorithm Begins here\n",
        "    for r in range(ROUTING_ITERATIONS):\n",
        "        with tf.variable_scope('iterations_' + str(r)):\n",
        "            c_ij = tf.nn.softmax(b_ij, axis=2) # [batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to back propagate gradient\n",
        "            if r == ROUTING_ITERATIONS - 1:\n",
        "                s_j = tf.multiply(c_ij, u_hat) # [batch_size, 1152, 10, 16, 1]\n",
        "                # then sum as per paper\n",
        "                s_j = tf.reduce_sum(s_j, axis=1, keep_dims=True) # [batch_size, 1, 10, 16, 1]\n",
        "\n",
        "                v_j = squash(s_j) # [batch_size, 1, 10, 16, 1]\n",
        "\n",
        "            elif r < ROUTING_ITERATIONS - 1:  # No backpropagation in these iterations\n",
        "                s_j = tf.multiply(c_ij, u_hat_stopped)\n",
        "                s_j = tf.reduce_sum(s_j, axis=1, keepdims=True)\n",
        "                v_j = squash(s_j)\n",
        "                v_j = tf.tile(v_j, [1, u.shape[1].value, 1, 1, 1]) # [batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "                # Multiplying in last two dimensions: [16, 1]^T x [16, 1] yields [1, 1]\n",
        "                u_hat_dot_v = tf.matmul(u_hat_stopped, v_j, transpose_a=True) # [batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                b_ij = tf.add(b_ij,u_hat_dot_v)\n",
        "    return tf.squeeze(v_j, axis=1) # [batch_size, 10, 16, 1]\n",
        "\n",
        "  \n",
        "def shuffle_data(x, y):\n",
        "    \"\"\" Shuffle the features and labels of input data\"\"\"\n",
        "    perm = np.arange(y.shape[0])\n",
        "    np.random.shuffle(perm)\n",
        "    shuffle_x = x[perm,:,:,:]\n",
        "    shuffle_y = y[perm]\n",
        "    return shuffle_x, shuffle_y\n",
        "\n",
        "\n",
        "def load_existing_details():\n",
        "    \"\"\"\n",
        "    This function loads the train and val files to continue training\n",
        "    :return: handles to train and val files and minimum validation loss\n",
        "    \"\"\"\n",
        "    train_path = RESULTS_DIR  + '/' + 'train.csv'\n",
        "    val_path = RESULTS_DIR + '/' + 'validation.csv'\n",
        "    # finding the minimum validation loss so far\n",
        "    f_val = open(val_path, 'r')\n",
        "    lines = f_val.readlines()\n",
        "    data = np.genfromtxt(lines[-1:], delimiter=',')\n",
        "    min_loss = np.min(data[1:, 2])\n",
        "    # loading the train and val files to continue training\n",
        "    train_file = open(train_path, 'a')\n",
        "    val_file = open(val_path, 'a')\n",
        "    return train_file, val_file, min_loss\n",
        "\n",
        "\n",
        "def eval_performance(sess, model, x, y):\n",
        "    '''\n",
        "    This function is mainly used to evaluate the accuracy on test and validation sets\n",
        "    :param sess: session\n",
        "    :param model: model to be used\n",
        "    :param x: images\n",
        "    :param y: labels\n",
        "    :return: returns the average accuracy and loss for the dataset\n",
        "    '''\n",
        "    acc_all = loss_all = np.array([])\n",
        "    num_batches = int(y.shape[0] / BATCH_SIZE)\n",
        "    for batch_num in range(num_batches):\n",
        "        start = batch_num * BATCH_SIZE\n",
        "        end = start + BATCH_SIZE\n",
        "        x_batch, y_batch = x[start:end], y[start:end]\n",
        "        acc_batch, loss_batch, prediction_batch = sess.run([model.accuracy, model.combined_loss, model.y_predicted],\n",
        "                                                     feed_dict={model.X: x_batch, model.Y: y_batch})\n",
        "        acc_all = np.append(acc_all, acc_batch)\n",
        "        loss_all = np.append(loss_all, loss_batch)\n",
        "    return np.mean(acc_all), np.mean(loss_all)\n",
        "\n",
        "def reconstruction(x, y, decoder_output, y_pred, n_samples):\n",
        "    '''\n",
        "    This function is used to reconstruct sample images for analysis\n",
        "    :param x: Images\n",
        "    :param y: Labels\n",
        "    :param decoder_output: output from decoder\n",
        "    :param y_pred: predictions from the model\n",
        "    :param n_samples: num images\n",
        "    :return: saves the reconstructed images\n",
        "    '''\n",
        "\n",
        "    sample_images = x.reshape(-1, IMG_WIDTH, IMG_HEIGHT)\n",
        "    decoded_image = decoder_output.reshape([-1, IMG_WIDTH, IMG_WIDTH])\n",
        "\n",
        "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(1, n_samples, i+ 1)\n",
        "        plt.imshow(sample_images[i], cmap=\"binary\")\n",
        "        plt.title(\"Label:\" + IMAGE_LABELS[np.argmax(y[i])])\n",
        "        plt.axis(\"off\")\n",
        "    fig.savefig(RESULTS_DIR + '/' + 'input_images.png')\n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(1, n_samples, i + 1)\n",
        "        plt.imshow(decoded_image[i], cmap=\"binary\")\n",
        "        plt.title(\"Prediction:\" + IMAGE_LABELS[y_pred[i]])\n",
        "        plt.axis(\"off\")\n",
        "    fig.savefig(RESULTS_DIR + '/' + 'decoder_images.png')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jMVKE4YWu81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet:\n",
        "    def __init__(self):\n",
        "        with tf.variable_scope('Input'):\n",
        "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS], dtype=tf.float32, name=\"X\")\n",
        "            self.Y = tf.placeholder(shape=[None, N_CLASSES], dtype=tf.float32, name=\"Y\")\n",
        "            self.mask_with_labels = tf.placeholder_with_default(False, shape=(), name=\"mask_with_labels\")\n",
        "\n",
        "        self.define_network()\n",
        "        self.define_loss()\n",
        "        self.define_accuracy()\n",
        "        self.define_optimizer()\n",
        "        self.summary_()\n",
        "\n",
        "    def define_network(self):\n",
        "        with tf.variable_scope('Conv1_layer', reuse=True):\n",
        "            conv1_layer = tf.layers.conv2d(self.X, name=\"conv1_layer\", **CONV1_LAYER_PARAMS) # [batch_size, 20, 20, 256]\n",
        "\n",
        "        with tf.variable_scope('PrimaryCaps_layer', reuse=True):\n",
        "            conv2_layer = tf.layers.conv2d(conv1_layer, name=\"conv2_layer\", **CONV2_LAYER_PARAMS) # [batch_size, 6, 6, 256]\n",
        "\n",
        "            primary_caps = tf.reshape(conv2_layer, (BATCH_SIZE, NCAPS_CAPS1, CAPS_DIM_CAPS1, 1), name=\"primary_caps\") # [batch_size, 1152, 8, 1]\n",
        "            primary_caps_output = squash(primary_caps, name=\"caps1_output\")\n",
        "            # [batch_size, 1152, 8, 1]\n",
        "\n",
        "        # DigitCaps layer, return [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer', reuse=True):\n",
        "            digitcaps_input = tf.reshape(primary_caps_output, shape=(BATCH_SIZE, NCAPS_CAPS1, 1, CAPS_DIM_CAPS1, 1)) # [batch_size, 1152, 1, 8, 1]\n",
        "            # [batch_size, 1152, 10, 1, 1]\n",
        "            self.digitcaps_output = routing(digitcaps_input) # [batch_size, 10, 16, 1]\n",
        "\n",
        "        # Decoder\n",
        "        with tf.variable_scope('Masking', reuse=True):\n",
        "            self.v_norm = tf.sqrt(tf.reduce_sum(tf.square(self.digitcaps_output), axis=2, keep_dims=True) + tf.keras.backend.epsilon())\n",
        "\n",
        "            predicted_class = tf.to_int32(tf.argmax(self.v_norm, axis=1)) #[batch_size, 10,1,1]\n",
        "            self.y_predicted = tf.reshape(predicted_class, shape=(BATCH_SIZE,))  #[batch_size]\n",
        "            y_predicted_one_hot = tf.one_hot(self.y_predicted, depth=NCAPS_CAPS2)  #[batch_size,10]  One hot operation\n",
        "\n",
        "            reconstruction_targets = tf.cond(self.mask_with_labels,  # condition\n",
        "                                      lambda: self.Y,  # if True (Training)\n",
        "                                      lambda: y_predicted_one_hot,  # if False (Test)\n",
        "                                      name=\"reconstruction_targets\")\n",
        "\n",
        "            digitcaps_output_masked = tf.multiply(tf.squeeze(self.digitcaps_output), tf.expand_dims(reconstruction_targets, -1)) # [batch_size, 10, 16]\n",
        "\n",
        "\n",
        "            #Flattening as suggested by the paper\n",
        "            decoder_input = tf.reshape(digitcaps_output_masked, [BATCH_SIZE, -1]) # [batch_size, 160]\n",
        "\n",
        "\n",
        "        with tf.variable_scope('Decoder', reuse=True):\n",
        "            fc1 = tf.layers.dense(decoder_input, layer1_size, activation=tf.nn.relu, name=\"FC1\") # [batch_size, 512]\n",
        "            fc2 = tf.layers.dense(fc1, layer2_size, activation=tf.nn.relu, name=\"FC2\") # [batch_size, 1024]\n",
        "            self.decoder_output = tf.layers.dense(fc2, output_size, activation=tf.nn.sigmoid, name=\"FC3\") # [batch_size, 784]\n",
        "\n",
        "\n",
        "    def define_loss(self):\n",
        "        # Margin Loss\n",
        "        with tf.variable_scope('Margin_Loss', reuse=True):\n",
        "            # max(0, m_plus-||v_c||)^2\n",
        "            positive_error = tf.square(tf.maximum(0., 0.9 - self.v_norm)) # [batch_size, 10, 1, 1]\n",
        "            # max(0, ||v_c||-m_minus)^2\n",
        "            negative_error = tf.square(tf.maximum(0., self.v_norm - 0.1)) # [batch_size, 10, 1, 1]\n",
        "            # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "            positive_error = tf.reshape(positive_error, shape=(BATCH_SIZE, -1))\n",
        "            negative_error = tf.reshape(negative_error, shape=(BATCH_SIZE, -1))\n",
        "\n",
        "            Loss_vec = self.Y * positive_error + 0.5 * (1- self.Y) * negative_error # [batch_size, 10]\n",
        "            self.margin_loss = tf.reduce_mean(tf.reduce_sum(Loss_vec, axis=1), name=\"margin_loss\")\n",
        "\n",
        "        # Reconstruction Loss\n",
        "        with tf.variable_scope('Reconstruction_Loss', reuse=True):\n",
        "            ground_truth = tf.reshape(self.X, shape=(BATCH_SIZE, -1))\n",
        "            self.reconstruction_loss = tf.reduce_mean(tf.square(self.decoder_output - ground_truth))\n",
        "\n",
        "        # Combined Loss\n",
        "        with tf.variable_scope('Combined_Loss', reuse=True):\n",
        "            self.combined_loss = self.margin_loss + 0.0005 * self.reconstruction_loss\n",
        "\n",
        "    def define_accuracy(self):\n",
        "        with tf.variable_scope('Accuracy', reuse=True):\n",
        "            correct_predictions = tf.equal(tf.to_int32(tf.argmax(self.Y, axis=1)), self.y_predicted)\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "    def define_optimizer(self):\n",
        "        with tf.variable_scope('Optimizer', reuse=True):\n",
        "            optimizer = tf.train.AdamOptimizer()\n",
        "            self.train_optimizer = optimizer.minimize(self.combined_loss, name=\"training_optimizer\")\n",
        "\n",
        "    def summary_(self):\n",
        "        reconstructed_image = tf.reshape(self.decoder_output, shape=(BATCH_SIZE, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS))\n",
        "        summary_list = [tf.summary.scalar('Loss/margin_loss', self.margin_loss),\n",
        "                        tf.summary.scalar('Loss/reconstruction_loss', self.reconstruction_loss),\n",
        "                        tf.summary.image('original', self.X),\n",
        "                        tf.summary.image('reconstructed', reconstructed_image)]\n",
        "        self.summary_ = tf.summary.merge(summary_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SL2DH4YWBOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "\n",
        "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def train(model):\n",
        "    global fd_train\n",
        "    x_train, y_train, x_valid, y_valid = load_data()\n",
        "    print('Data set Loaded')\n",
        "    num_batches = int(y_train.shape[0] / BATCH_SIZE)\n",
        "    if not os.path.exists(CHECKPOINT_PATH_DIR):\n",
        "        os.makedirs(CHECKPOINT_PATH_DIR)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        if RESTORE_TRAINING:\n",
        "            saver = tf.train.Saver()\n",
        "            ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            print('Model Loaded')\n",
        "            start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1])\n",
        "            train_file, val_file, best_loss_val = load_existing_details()\n",
        "        else:\n",
        "            saver = tf.train.Saver(tf.global_variables())\n",
        "            tf.global_variables_initializer().run()\n",
        "            print('All variables initialized')\n",
        "            #train_file, val_file = write_progress('train')\n",
        "            start_epoch = 0\n",
        "            best_loss_val = np.infty\n",
        "        print('Training Starts')\n",
        "        acc_batch_all = loss_batch_all = np.array([])\n",
        "        train_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
        "        for epoch in range(start_epoch, EPOCHS):\n",
        "            # Shuffle the input data\n",
        "            #x_train, y_train = shuffle_data(x_train, y_train)\n",
        "            for step in range(num_batches):\n",
        "                start = step * BATCH_SIZE\n",
        "                end = (step + 1) * BATCH_SIZE\n",
        "                global_step = epoch * num_batches + step\n",
        "                x_batch, y_batch = x_train[start:end], y_train[start:end]\n",
        "                feed_dict_batch = {model.X: x_batch, model.Y: y_batch, model.mask_with_labels: True}\n",
        "                if not (step % 100):\n",
        "                    _, acc_batch, loss_batch, summary_ = sess.run([model.train_optimizer, model.accuracy,\n",
        "                                                                     model.combined_loss, model.summary_],\n",
        "                                                                    feed_dict=feed_dict_batch)\n",
        "                    train_writer.add_summary(summary_, global_step)\n",
        "                    acc_batch_all = np.append(acc_batch_all, acc_batch)\n",
        "                    loss_batch_all = np.append(loss_batch_all, loss_batch)\n",
        "                    mean_acc,mean_loss = np.mean(acc_batch_all),np.mean(loss_batch_all)\n",
        "                    summary_ = tf.Summary(value=[tf.Summary.Value(tag='Accuracy', simple_value=mean_acc)])\n",
        "                    train_writer.add_summary(summary_, global_step)\n",
        "                    summary_ = tf.Summary(value=[tf.Summary.Value(tag='Loss/combined_loss', simple_value=mean_loss)])\n",
        "                    train_writer.add_summary(summary_, global_step)\n",
        "\n",
        "                    train_file.write(str(global_step) + ',' + str(mean_acc) + ',' + str(mean_loss) + \"\\n\")\n",
        "                    train_file.flush()\n",
        "                    print(\"  Batch #{0}, Epoch: #{1}, Mean Training loss: {2:.4f}, Mean Training accuracy: {3:.01%}\".format(\n",
        "                        step, (epoch+1), mean_loss, mean_acc))\n",
        "                    acc_batch_all = loss_batch_all = np.array([])\n",
        "                else:\n",
        "                    _, acc_batch, loss_batch = sess.run([model.train_optimizer, model.accuracy, model.combined_loss],\n",
        "                                                        feed_dict=feed_dict_batch)\n",
        "                    acc_batch_all = np.append(acc_batch_all, acc_batch)\n",
        "                    loss_batch_all = np.append(loss_batch_all, loss_batch)\n",
        "\n",
        "            # Validation metrics after each EPOCH\n",
        "            acc_val, loss_val = eval_performance(sess, model, x_valid, y_valid)\n",
        "            val_file.write(str(epoch + 1) + ',' + str(acc_val) + ',' + str(loss_val) + '\\n')\n",
        "            val_file.flush()\n",
        "            print(\"\\rEpoch: {}  Mean Train Accuracy: {:.4f}% ,Mean Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
        "                epoch + 1, mean_acc * 100, acc_val * 100, loss_val,\n",
        "                \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
        "\n",
        "            # Saving the improved model\n",
        "            if loss_val < best_loss_val:\n",
        "                saver.save(sess, CHECKPOINT_PATH_DIR + '/model.tfmodel', global_step=epoch + 1)\n",
        "                best_loss_val = loss_val\n",
        "        train_file.close()\n",
        "        val_file.close()\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    x_test, y_test = load_data(load_type='test')\n",
        "    print('Loaded the test dataset')\n",
        "    test_file = write_progress('test')\n",
        "    saver = tf.train.Saver()\n",
        "    ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "        print('Model Loaded')\n",
        "        acc_test, loss_test = eval_performance(sess, model, x_test, y_test)\n",
        "        test_file.write(str(acc_test) + ',' + str(loss_test) + '\\n')\n",
        "        test_file.flush()\n",
        "        print('-----------------------------------------------------------------------------')\n",
        "        print(\"Test Set Loss: {0:.4f}, Test Set Accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
        "\n",
        "\n",
        "def reconstruct_sample(model, n_samples=5):\n",
        "    x_test, y_test = load_data(load_type='test')\n",
        "    sample_images, sample_labels = x_test[:BATCH_SIZE], y_test[:BATCH_SIZE]\n",
        "    saver = tf.train.Saver()\n",
        "    ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "        feed_dict_samples = {model.X: sample_images, model.Y: sample_labels}\n",
        "        decoder_out, y_predicted = sess.run([model.decoder_output, model.y_predicted],\n",
        "                                       feed_dict=feed_dict_samples)\n",
        "    reconstruction(sample_images, sample_labels, decoder_out, y_predicted, n_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wamYl-8SXggx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "f9278ec9-4b9e-4ba5-f2b0-a6b9c0834c48"
      },
      "source": [
        "N_CLASSES = 10\n",
        "IMG_WIDTH = 28\n",
        "IMG_HEIGHT = 28\n",
        "N_CHANNELS = 1                 # Number of Input Channels\n",
        "IMAGE_LABELS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "\n",
        "\n",
        "## Model Parameters\n",
        "CONV1_LAYER_PARAMS = {\"filters\": 256,\n",
        "                      \"kernel_size\": 9,\n",
        "                      \"activation\": tf.nn.relu,\n",
        "                      \"padding\": \"valid\",\n",
        "                      \"strides\": 1}\n",
        "\n",
        "# Parameters of PrimaryCaps_layer\n",
        "MAPS_CAPS1 = 32\n",
        "NCAPS_CAPS1 = MAPS_CAPS1*6*6  # Total number of primary capsules = 1152\n",
        "CAPS_DIM_CAPS1 = 8            # Dimensions of each capsule\n",
        "\n",
        "CONV2_LAYER_PARAMS = {\"filters\": MAPS_CAPS1 * CAPS_DIM_CAPS1,  # Total Convolutional Filters = 256\n",
        "                      \"kernel_size\": 9,\n",
        "                      \"strides\": 2,\n",
        "                      \"padding\": \"valid\",\n",
        "                      \"activation\": tf.nn.relu}\n",
        "\n",
        "# Parameters of DigitCaps_layer\n",
        "NCAPS_CAPS2 = 10\n",
        "CAPS_DIM_CAPS2 = 16           # Dimension of each capsule in layer 2\n",
        "\n",
        "# Decoder Parameters\n",
        "layer1_size = 512\n",
        "layer2_size = 1024\n",
        "output_size = IMG_WIDTH* IMG_HEIGHT\n",
        "\n",
        "## Loss\n",
        "\n",
        "# Margin Loss\n",
        "M_PLUS = 0.9\n",
        "M_MINUS= 0.1\n",
        "LAMBDA = 0.5\n",
        "\n",
        "# Reconstruction Loss\n",
        "ALPHA = 0.0005\n",
        "\n",
        "# Training Params\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "ROUTING_ITERATIONS = 3    # Routing Iterations\n",
        "STDEV = 0.01  # STDEV for Weight Initialization\n",
        "\n",
        "\n",
        "## Environment and Save Directories\n",
        "RESTORE_TRAINING = False            # Restores the trained model\n",
        "CHECKPOINT_PATH_DIR = './model_dir'\n",
        "LOG_DIR = './logs/'\n",
        "RESULTS_DIR = './results/'\n",
        "STEPS_TO_SAVE = 100                 # Frequency (in steps) of saving the train result\n",
        "\n",
        "## Visualization Parameters\n",
        "N_SAMPLES = 3                       # No. of Samples Images to Save\n",
        "\n",
        "# Train the model and evaluate on test set\n",
        "model = CapsNet()\n",
        "print (\"Step1: Train\")\n",
        "train(model)\n",
        "print(\"Step2: Testing the performance of model on the Test Set\")\n",
        "test(model)\n",
        "print (\"Step3: Reconstructing some sample images\")\n",
        "reconstruct_sample(model,n_samples =3)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step1: Train\n",
            "Data set Loaded\n",
            "All variables initialized\n",
            "Training Starts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-4c9b91ad86ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCapsNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Step1: Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step2: Testing the performance of model on the Test Set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-88658ad062b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     _, acc_batch, loss_batch, summary_ = sess.run([model.train_optimizer, model.accuracy,\n\u001b[1;32m     44\u001b[0m                                                                      model.combined_loss, model.summary_],\n\u001b[0;32m---> 45\u001b[0;31m                                                                     feed_dict=feed_dict_batch)\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0macc_batch_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_batch_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128, 28, 28) for Tensor 'Input_10/X:0', which has shape '(?, 28, 28, 1)'"
          ]
        }
      ]
    }
  ]
}